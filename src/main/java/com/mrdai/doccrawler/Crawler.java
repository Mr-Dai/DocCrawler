package com.mrdai.doccrawler;

import com.mrdai.markj.*;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.IOException;
import java.net.URL;
import java.nio.file.Files;
import java.nio.file.Path;
import java.nio.file.Paths;
import java.util.*;

import static com.mrdai.doccrawler.util.NodeUtils.hasId;
import static java.nio.file.StandardOpenOption.WRITE;

public class Crawler {
    private static final Logger LOG = LoggerFactory.getLogger(Crawler.class);

    public void crawl(String seedUrlStr, String refSelector, String titleSelector, String contentSelector) throws IOException {
        LOG.debug("seedUrl = `{}`, refSelector = `{}`, contentSelector = `{}`, titleSelector = `{}`",
            seedUrlStr, refSelector, contentSelector, titleSelector);

        URL seedUrl = new URL(seedUrlStr);
        Document document = Jsoup.parse(new URL(seedUrlStr), 0);
        Elements refs = document.select(refSelector);
        LOG.info("Found {} refs.", refs.size());

        List<String> linksToVisit = new ArrayList<>(refs.size());
        for (Element ref : refs) {
            String link = String.format("%s://%s%s", seedUrl.getProtocol(), seedUrl.getHost(), ref.attr("href"));
            LOG.debug("`{}`", link);
            linksToVisit.add(link);
        }

        Map<String, Element> chapters = new LinkedHashMap<>();
        for (String link : linksToVisit)
            download(link, titleSelector, contentSelector, chapters);

        MarkdownParser parser = new JenkovParser();
        List<MarkdownNode> nodes = new LinkedList<>();

        parser.ignoreElement(hasId("pageToc"));
        parser.ignoreElement(hasId("lastUpdate"));
        parser.ignoreElement(hasId("next"));
        parser.ignoreElement(hasId("bottomSocial"));
        parser.fix();

        for (Map.Entry<String, Element> entry : chapters.entrySet()) {
            nodes.add(new Header(1, entry.getKey()));
            entry.getValue().childNodes().forEach((n) -> nodes.add(parser.parse(n)));
        }

        ParagraphNode copyright = new ParagraphNode();
        URL hostHome = new URL(String.format("%s://%s/", seedUrl.getProtocol(), seedUrl.getHost()));
        Document doc = Jsoup.parse(hostHome, 0);
        copyright.addChild(new PlainText("The page is crawled from "));
        copyright.addChild(new LinkNode(doc.title(), seedUrlStr));
        copyright.addChild(new PlainText(" and generated by "));
        copyright.addChild(new LinkNode("DocCrawler", "https://github.com/Mr-Dai/DocCrawler"));

        nodes.add(0, copyright);

        StringBuilder builder = new StringBuilder();
        for (MarkdownNode node : nodes)
            builder.append(node.toMarkdown());
        Path sumPath = Paths.get("nio.md");
        Files.deleteIfExists(sumPath);
        Files.createFile(sumPath);
        Files.write(sumPath, builder.toString().getBytes(), WRITE);
    }

    private void download(String link, String titleSelector, String contentSelector,
                          Map<String, Element> chapters) throws IOException {
        LOG.info("Parsing `{}`...", link);
        Document document = Jsoup.parse(new URL(link), 0);

        LOG.debug("Selecting title with selector `{}`", titleSelector);
        String title = document.select(titleSelector).first().text();
        LOG.info("Got page with title `{}`.", title);

        Element content = document.select(contentSelector).first();

        chapters.put(title, content);
    }

    public static void main(String[] args) throws IOException {
        Crawler crawler = new Crawler();
        crawler.crawl(args[0], args[1], args[2], args[3]);
    }
}
